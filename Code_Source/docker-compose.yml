services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    networks:
      - kafka-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    networks:
      - kafka-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://host.docker.internal:29092
      KAFKA_LISTENER_NAME_PLAINTEXT_HOST: INSECURE_PLAINTEXT
      KAFKA_LISTENER_NAME_PLAINTEXT: INSECURE_PLAINTEXT
      KAFKA_LISTENER_SECURITY_PROTOCOL: PLAINTEXT
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_MS: 3600000
      KAFKA_LOG_SEGMENT_DELETE_DELAY_MS: 60000
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka:9092"]
      interval: 10s
      timeout: 5s
      retries: 10


  hadoop:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    networks:
      - kafka-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "9870:9870"
      - "8020:8020"
    environment:
      - CLUSTER_NAME=thermoalert
    volumes:
      - hadoop_data:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    depends_on:
      - hadoop
    networks:
      - kafka-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - CLUSTER_NAME=thermoalert
      - CORE_CONF_fs_defaultFS=hdfs://hadoop:8020
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_client_use_datanode_hostname=true
    volumes:
      - datanode_data:/hadoop/dfs/data


  spark:
    image: bitnami/spark:3.3
    networks:
      - kafka-network
    volumes:
      - ./spark-app:/app
    extra_hosts:
      - "host.docker.internal:host-gateway"

  sensor:
    build: ./sensor
    networks:
      - kafka-network
    depends_on:
      kafka:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"

  hdfs-consumer:
    build: ./hdfs-consumer
    networks:
      - kafka-network
    depends_on:
      - hadoop
      - kafka
    extra_hosts:
      - "host.docker.internal:host-gateway"

  alert-consumer:
    build: ./alert-consumer
    networks:
      - kafka-network
    depends_on:
      - kafka
    extra_hosts:
      - "host.docker.internal:host-gateway"

  spark-consumer:
    build: ./spark-app
    networks:
      - kafka-network
    depends_on:
      - kafka
    environment:
      - PYSPARK_SUBMIT_ARGS=--conf spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.4 pyspark-shell
    volumes:
      - ./spark-app:/app
    working_dir: /app
    command: ["spark-submit", "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.4", "/app/streaming_alerts.py"]

  interface:
    build: ./interface
    ports:
      - "5000:5000"
    networks:
      - kafka-network
    depends_on:
      - kafka

volumes:
  hadoop_data:
  datanode_data:

networks:
  kafka-network:
    driver: bridge
